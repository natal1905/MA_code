{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/nb/PycharmProjects/pythonProject/venv3/lib/python3.8/site-packages/pandoc/definitions/1.16.hs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1be85c6afdb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv3/lib/python3.8/site-packages/pandoc/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mabout\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv3/lib/python3.8/site-packages/pandoc/types.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mglobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_types_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m \u001b[0mmake_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv3/lib/python3.8/site-packages/pandoc/types.py\u001b[0m in \u001b[0;36mmake_types\u001b[0;34m(defs)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mdefs_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"definitions/{0}.hs\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mdefs_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pandoc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefs_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefs_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mdefs_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefs_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv3/lib/python3.8/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mresource_string\u001b[0;34m(self, package_or_requirement, resource_name)\u001b[0m\n\u001b[1;32m   1140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mresource_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_or_requirement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0;34m\"\"\"Return specified resource as a string\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m         return get_provider(package_or_requirement).get_resource_string(\n\u001b[0m\u001b[1;32m   1143\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m         )\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv3/lib/python3.8/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_resource_string\u001b[0;34m(self, manager, resource_name)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_resource_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhas_resource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv3/lib/python3.8/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_get\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1611\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1612\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/nb/PycharmProjects/pythonProject/venv3/lib/python3.8/site-packages/pandoc/definitions/1.16.hs'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandoc\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "from nltk import Tree\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_nlp = spacy.load('de_core_news_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "use_lsk = False\n",
    "n_splits = 10\n",
    "\n",
    "output_path = r\"C:\\Users\\natal\\Desktop\\Korpus\\debug_1812\"\n",
    "output_path = r'D:\\Natalie\\13_korpus\\run_bote_210216'\n",
    "#output_path = r'D:\\Natalie\\13_korpus\\run_mag_210216'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gazetteer Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gazetteer_LIN = r\"D:\\Natalie\\07_gazetteer\\swissNames3D_LIN.csv\"\n",
    "gazetteer_PKT = r\"D:\\Natalie\\07_gazetteer\\swissNames3D_PKT.csv\"\n",
    "gazetteer_PLY = r\"D:\\Natalie\\07_gazetteer\\swissNames3D_PLY.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files_eval= r\"F:\\Korpus\\pages_Bote\"\n",
    "#text_files_eval= r\"F:\\Korpus\\pages_Magazin\"\n",
    "#text_files_eval= r\"C:\\Users\\natal\\Desktop\\Korpus\\test_pages\"\n",
    "text_lsk= r\"F:\\Korpus\\LSK_Text\"\n",
    "\n",
    "korpus_data = text_files_eval\n",
    "\n",
    "if use_lsk:\n",
    "    korpus_data = text_lsk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10043/10043 [00:46<00:00, 218.30it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9830/9830 [00:46<00:00, 213.60it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9969/9969 [01:44<00:00, 95.30it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2888/2888 [00:26<00:00, 108.20it/s]\n"
     ]
    }
   ],
   "source": [
    "abbreviations = [\n",
    "    (\"Nr.\", \"Nr\"),\n",
    "    (\"z.B.\", \"zB\"),\n",
    "    (\"zB.\", \"zB\"), \n",
    "    (\"St.\", \"St\")\n",
    "]\n",
    "\n",
    "sentences = []\n",
    "for ordner in os.listdir(korpus_data):\n",
    "    for text_file in tqdm(os.listdir(os.path.join(korpus_data, ordner))):\n",
    "        with open(os.path.join(korpus_data, ordner, text_file), encoding='utf-8') as file:\n",
    "            data = file.read()\n",
    "            for abbr in abbreviations:\n",
    "                data = data.replace(abbr[0], abbr[1])\n",
    "            file_sentences = re.split(r' *[\\.\\?!] *', data) # orig: split nach Punkten\n",
    "            sentences += file_sentences\n",
    "    if debug:\n",
    "        #break\n",
    "        pass\n",
    "        \n",
    "if debug:\n",
    "    sentences = sentences[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5443296\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences)) #alle Sätze: 5'522'722\n",
    "print(type(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaz_to_df(gaz):\n",
    "    df = pd.read_csv(gaz, sep=';', encoding='utf-8', usecols=[\"NAME\", \"UUID\", \"OBJEKTART\", \"POINT_X\", \"POINT_Y\", \"POINT_Z\", \"Type\"])\n",
    "    #print(df.columns)\n",
    "    return df\n",
    "\n",
    "gazetteer_LIN_df = gaz_to_df(gazetteer_LIN)\n",
    "gazetteer_PKT_df = gaz_to_df(gazetteer_PKT)\n",
    "gazetteer_PLY_df = gaz_to_df(gazetteer_PLY)\n",
    "gazetteer_all = pd.concat([gazetteer_LIN_df, gazetteer_PKT_df, gazetteer_PLY_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gazetteer_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_ner(X):\n",
    "    entities=[]\n",
    "    sentences=[]\n",
    "    #perform named entity recognition using spacy\n",
    "    for line in tqdm(X):\n",
    "        text_ner = base_nlp(line)\n",
    "        temp_ents = []\n",
    "        for ent in text_ner.ents:\n",
    "            if ent.label_ == \"LOC\":\n",
    "                temp_ents.append(ent.text)\n",
    "                if len(sentences)==0 or sentences[-1] != line:\n",
    "                    sentences.append(line)\n",
    "        if temp_ents:\n",
    "            entities.append(temp_ents)\n",
    "    #falls die entity als Location identifiziert wird, soll der dazugehörige Satz gespeichert werden.\n",
    "    return entities, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_gazetteer_rules(X, gazetteer):\n",
    "    sentences_winfo=[]\n",
    "    sentences=[]\n",
    "    toponym_nr_ids={}\n",
    "    toponym_count={}\n",
    "    topo_id_count={}\n",
    "    topo_in_sen=[]\n",
    "    \n",
    "    endings = [\"er\", \"in\", \"innen\", \"Innen\",\"s\", \"en\",\"ers\", \"erisch\",\"isch\",\"e\", \"n\", \"ig\", \"es\", \"ieren\", \"eln\", \"ung\"]\n",
    "    pattern = \"nm4you\"\n",
    "    for uuid, topo, objektart, typus, coord_x, coord_y, coord_z in zip(gazetteer.iloc[:,0], gazetteer.iloc[:,2], gazetteer.iloc[:,1], gazetteer.iloc[:,6], gazetteer.iloc[:,3], gazetteer.iloc[:,4], gazetteer.iloc[:,5]):    \n",
    "        if topo not in toponym_nr_ids:\n",
    "            toponym_nr_ids[topo] = [{\"uuid\": uuid, \"objektart\": objektart, \"geom\": typus, \"coords\": (coord_x, coord_y, coord_z)}]\n",
    "            pattern += f\"|{topo}(\\\\b\"\n",
    "            for ending in endings:\n",
    "                pattern += f\"|{ending}\\\\b\"\n",
    "            pattern += \")\"   \n",
    "        else:\n",
    "            toponym_nr_ids[topo].append({\"uuid\": uuid, \"objektart\": objektart, \"geom\": typus, \"coords\": (coord_x, coord_y, coord_z)})\n",
    "\n",
    "    for sen in tqdm(X):\n",
    "        document = re.sub(r'\\W', ' ', str(sen))\n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)  \n",
    "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)  \n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I) \n",
    "        document = re.sub(r'^b\\s+', '', document) \n",
    "        sen = document\n",
    "        \n",
    "        results = list(re.finditer(pattern, sen))\n",
    "        \n",
    "        if results:\n",
    "            sentences.append(sen)\n",
    "            topo_list = []\n",
    "            info_dict = {\"sentence\": sen, \"topo_infos\": []}\n",
    "            for res in results:\n",
    "                result = res.string[res.start():res.end()]\n",
    "                topo_list.append(result)\n",
    "                for topo in toponym_nr_ids:\n",
    "                    if any([topo + ending == result for ending in endings + [\"\"]]):\n",
    "                        new_topo_info = {\"topo\": topo, \"info\": toponym_nr_ids[topo]}\n",
    "                        if not new_topo_info in info_dict[\"topo_infos\"]:\n",
    "                            info_dict[\"topo_infos\"].append(new_topo_info)\n",
    "                        if topo not in topo_id_count:\n",
    "                            topo_id_count[topo]=len(toponym_nr_ids[topo])\n",
    "                if result not in toponym_count:\n",
    "                    toponym_count[result] = 1\n",
    "                else:\n",
    "                    toponym_count[result] += 1\n",
    "            topo_in_sen.append(topo_list)\n",
    "            sentences_winfo.append(info_dict)\n",
    "  \n",
    "    return sentences, toponym_nr_ids, toponym_count, topo_id_count, topo_in_sen, sentences_winfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Topo Sentences to PKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitsize = int(len(sentences)/n_splits)+1\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "for i in range(n_splits):\n",
    "    nc_split = sentences[i*splitsize:(i+1)*splitsize]\n",
    "    #nc_split = nc_split[:10]\n",
    "    print(\"perfom ner \", i)\n",
    "    entities, saetze_ner = perform_ner(X=nc_split)\n",
    "    print(\"perform regex \", i)\n",
    "    saetze, toponym_ids, toponym_counts, topo_id_counts, topo_in_sen, sentences_winfo = search_gazetteer_rules(X=saetze_ner, gazetteer=gazetteer_all)\n",
    "    \n",
    "    pickle_data = {\n",
    "        \"entities\":entities , \n",
    "        \"saetze_ner\":saetze_ner, \n",
    "        \"saetze\":saetze, \n",
    "        \"toponym_ids\":toponym_ids, \n",
    "        \"toponym_counts\":toponym_counts, \n",
    "        \"topo_id_counts\":topo_id_counts, \n",
    "        \"topo_in_sen\":topo_in_sen, \n",
    "        \"sentences_winfo\":sentences_winfo\n",
    "    }\n",
    "    with open(f\"{output_path}/temp_topo_{i}.pkl\", 'wb') as file:\n",
    "        pickle.dump(pickle_data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load <sentences_winfo> again from PKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_topos_in_corpus=[]\n",
    "\n",
    "for temp_topo_file in os.listdir(output_path):\n",
    "    if temp_topo_file.startswith(\"temp_topo\"):\n",
    "        with open(os.path.join(output_path, temp_topo_file), 'rb') as fileo:\n",
    "            print(os.path.join(output_path, temp_topo_file))\n",
    "            data_frompickle = pickle.load(fileo)\n",
    "            all_topos_in_corpus.append(data_frompickle['sentences_winfo'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topos_in_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Topos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UUID = []\n",
    "TOPO = []\n",
    "for row in all_topos_in_corpus:\n",
    "    for sen in row:\n",
    "        for topo_info in sen['topo_infos']:\n",
    "            for info in topo_info['info']:\n",
    "                UUID.append(info['uuid'])\n",
    "                TOPO.append(topo_info['topo'])\n",
    "                break\n",
    "\n",
    "\n",
    "with open(f\"{output_path}/all_topos_in_corpus.csv\", 'w', encoding=\"utf-8\") as output_file:\n",
    "    csv_out=csv.writer(output_file)\n",
    "    csv_out.writerow(['topo','uuid'])\n",
    "    for topo, uuid in zip(TOPO, UUID):\n",
    "        csv_out.writerow([topo, uuid]) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(toponym_counts.items())\n",
    "print(len(toponym_counts))\n",
    "print(len(toponym_ids))\n",
    "print(len(topo_id_counts))\n",
    "print(len(saetze))\n",
    "print(len(topo_in_sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topo_id_counts.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After Finding LSB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nicht mehr brauchen, wegen Simons tollem Algorithmus im combine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toponym NER --> LSB --> Toponym mit UUID dranspeichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## für ein File, das schon pro Linie einen Satz enthält\n",
    "\n",
    "eval_pre=[]\n",
    "debug_info=[]\n",
    "classifications=[]\n",
    "saetze=[]\n",
    "\n",
    "for file in os.listdir(output_path):\n",
    "    if file.startswith('temp_lsb'):\n",
    "        with open(os.path.join(output_path, file), 'rb') as fileo:\n",
    "            print(os.path.join(output_path, file))\n",
    "            data = pickle.load(fileo)\n",
    "            eval_pre += data[\"eval_pre\"]\n",
    "            debug_info += data[\"debug_info\"]\n",
    "            classifications.append(data[\"eval_pred\"])\n",
    "        if debug:\n",
    "            break\n",
    "    \n",
    "classifications = np.concatenate(classifications)\n",
    "\n",
    "for res, sen, debinfo in sorted(zip(classifications, eval_pre, debug_info)):\n",
    "        if res == 1:\n",
    "            saetze.append(sen+\"\\n\")\n",
    "            \n",
    "all_saetze, all_toponym_ids, all_toponym_counts, all_topo_id_counts, all_topo_in_sen, all_sentences_winfo = search_gazetteer_rules(X=saetze, gazetteer=gazetteer_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a file where data need to be stored\n",
    "file = open(f\"{output_path}/temp_both.pkl\", 'wb')\n",
    "# dump information to the file\n",
    "pickle.dump(all_sentences_winfo, file)\n",
    "# close the file\n",
    "file.close()\n",
    "print(file)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sentences_winfo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambig = 0\n",
    "unique = 0\n",
    "topo_id_counts_ambig=[]\n",
    "for topo, id_count in topo_id_counts.items():\n",
    "    print(topo)\n",
    "    print(id_count)\n",
    "    if id_count > 1:\n",
    "        ambig += 1\n",
    "        topo_id_counts_ambig.append(topo)\n",
    "    else:\n",
    "        unique += 1\n",
    "print(ambig)\n",
    "print(unique)\n",
    "print(len(topo_id_counts))\n",
    "print(ambig/len(topo_id_counts))\n",
    "print(unique/len(topo_id_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambig_1topo=0\n",
    "ambig_mehretopo=0\n",
    "\n",
    "for topo, sentence in sorted(zip(topo_in_sen, saetze)):\n",
    "    if len(topo) > 1:\n",
    "        ambig_mehretopo += 1\n",
    "        for t in topo:\n",
    "            if t in topo_id_counts_ambig:\n",
    "                print(\"Mehrere Topo: \", t, \": \\n\",  sentence) \n",
    "    else:\n",
    "        ambig_1topo += 1\n",
    "        for t in topo:\n",
    "            if t in topo_id_counts_ambig:\n",
    "                print(\"Nur 1 Topo: \", t, \": \\n\", sentence)\n",
    "print(ambig_1topo)\n",
    "print(ambig_mehretopo)\n",
    "print(ambig_1topo/(ambig_1topo+ambig_mehretopo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(topo_id_counts.items())\n",
    "print(sorted(topo_id_counts.items(), key=lambda kv: kv[1]))\n",
    "#print(saetze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision & Recall - Vergleich zweier Methoden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files_orig = r\"C:\\Users\\natal\\Desktop\\Studium\\01 Masterarbeit\\08_Evaluation\\original_2Round\"\n",
    "\n",
    "text_files_annot = r\"C:\\Users\\natal\\Desktop\\Studium\\01 Masterarbeit\\08_Evaluation\\annotiert_2Round\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_context = []\n",
    "for text_file in tqdm(os.listdir(text_files_annot)):\n",
    "    with open(os.path.join(text_files_annot, text_file), encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "        #no_context.append('\\n'.join(file.readlines()))\n",
    "        sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', data)\n",
    "        no_context += sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annot(X, marker=\"xxx\"):\n",
    "    marked_list = []\n",
    "    for line in X:\n",
    "        for word in line.replace(\"\\n\", \" \").split(\" \"):\n",
    "            if marker in word:\n",
    "                for part in word.split(marker):\n",
    "                    part_stripped = part.rstrip(\"-\\n/?)“„«,.(:\")\n",
    "                    if part_stripped:\n",
    "                        marked_list.append(part_stripped)\n",
    "    return marked_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_topos(marked_list):\n",
    "    topo_dict = {}\n",
    "    for sublist in marked_list:\n",
    "        for topo in sublist:\n",
    "            if topo not in topo_dict:\n",
    "                topo_dict[topo] = 1\n",
    "            else:\n",
    "                topo_dict[topo] += 1\n",
    "    return topo_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Annotierte Docs\n",
    "marked_list = load_annot(X=no_context, marker=\"xxx\")\n",
    "ground_truth_dict = count_topos(marked_list)\n",
    "\n",
    "print(len(ground_truth_dict))\n",
    "for k in sorted(ground_truth_dict.keys()):\n",
    "    print(k, ground_truth_dict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Originale Docs - NER und Gazetter mit Rules\n",
    "no_context = []\n",
    "for text_file in tqdm(os.listdir(text_files_orig)):\n",
    "    with open(os.path.join(text_files_orig, text_file), encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "        #no_context.append('\\n'.join(file.readlines()))\n",
    "        sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', data)\n",
    "        no_context += sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ner_loc_list_temp = perform_ner(X=no_context)\n",
    "saetze, toponym_ids, toponym_counts, topo_id_counts, ner_loc_list = search_gazetteer_rules(X=ner_loc_list_temp, gazetteer=gazetteer_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topo, satz in zip(ner_loc_list, saetze):\n",
    "    print(topo, \" in \", satz, \"-----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topos_count_dict = count_topos(ner_loc_list)\n",
    "\n",
    "print(len(topos_count_dict))\n",
    "for k in sorted(topos_count_dict.keys()):\n",
    "    print(k, topos_count_dict[k], \",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Originale Docs - nur Gazetteer mit Rules, ohne NER\n",
    "   \n",
    "saetze, toponym_ids, toponym_counts, topo_id_counts, gaz_loc_list = search_gazetteer_rules(X=no_context, gazetteer=gazetteer_all)\n",
    "\n",
    "topos_count_dict = count_topos(gaz_loc_list)\n",
    "\n",
    "print(len(topos_count_dict))\n",
    "for k in sorted(topos_count_dict.keys()):\n",
    "    print(k, topos_count_dict[k], \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Precision: wieviele Topos sind relevant von allen Resultaten\n",
    "### True Positives / Trues Positives + False Positives\n",
    "def precision(results, ground_truth):\n",
    "    true_p = 0\n",
    "    false_p = 0\n",
    "    for topo in ground_truth:\n",
    "        if topo in results:\n",
    "            true_p = true_p + min(ground_truth[topo], results[topo])\n",
    "    for topo in results:\n",
    "        if topo not in ground_truth:\n",
    "            false_p += results[topo]\n",
    "        else: \n",
    "            false_p += min(results[topo] - ground_truth[topo], 0)\n",
    "    return true_p/(true_p + false_p)\n",
    "\n",
    "#### Recall: wieviele Topos, die ich brauchen kann, sind in den Resultaten\n",
    "### True Positives / True Positives + False Negatives\n",
    "def recall(results, ground_truth):\n",
    "    true_p = 0\n",
    "    for topo in ground_truth:\n",
    "        if topo in results:\n",
    "            true_p = true_p + min(ground_truth[topo], results[topo])\n",
    "    return true_p/sum(ground_truth.values())\n",
    "\n",
    "print(precision(topos_count_dict, ground_truth_dict))\n",
    "print(recall(topos_count_dict, ground_truth_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER & Gazetteer\n",
    "0.7362110311750599\n",
    "0.6303901437371663\n",
    "\n",
    "# Nur Gazetteer\n",
    "0.568760611205433\n",
    "0.6878850102669405"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, t, tc, tic, tis, res = search_gazetteer_rules(X=[\"Hallo  Alpthal Birchli, Birchli, Birchli, Hoch Ybrig und Hoch-Ybrig\"], gazetteer=gazetteer_all)\n",
    "res\n",
    "with open(r\"C:\\Users\\natal\\Desktop\\test.json\", \"w\") as write_file:\n",
    "    json.dump(res, write_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
